{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Automatic Differentiation with ``torch.autograd``\n",
    "\n",
    "When training neural networks, the most frequently used algorithm is **back propagation**.   \n",
    "\n",
    "In this algorithm, parameters (model weights) are adjusted   \n",
    "\n",
    "according to the **gradient** of the loss function with respect to the given parameter.  \n",
    "adjusted : 조절됨 - 손실 함수의 기울기에 따라  \n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called ``torch.autograd``.   \n",
    "자동미분    \n",
    "It supports automatic computation of gradient for any computational graph.  \n",
    "\n",
    "Consider the simplest one-layer neural network, with input ``x``,\n",
    "parameters ``w`` and ``b``, and some loss function.   \n",
    "It can be defined in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "# 크기가 5인 1로 채워진 텐서\n",
    "\n",
    "y = torch.zeros(3)  # expected output\n",
    "# 크기가 3인 0으로 채워진 텐서\n",
    "\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "# 크기가 (5, 3)인 무작위로 초기화된 텐서\n",
    "# 모델의 weight\n",
    "# requires_grad=True : 이 텐서에 대한 그래디언트 계산이 필요하다는 것을 나타냄\n",
    "\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "# 크기가 3인 무작위로 초기화된 텐서 \n",
    "# 모델의 bias\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "# torch.matmul(x, w): 입력 텐서 x와, weight 텐서 w 간의 행렬 곱셈을 수행\n",
    "# x는 1차원 텐서이고, w는 2차원 텐서이므로, 내적(inner product)과 동일\n",
    "# 결과로 나오는 텐서는 1차원 텐서\n",
    "# + b 부분은 행렬 곱셈의 결과에 편향 b를 더함\n",
    "# 편향은 각각의 출력에 더해져서 최종적인 출력 z를 얻게 됨\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "# logit 값 z와 실제 타겟 y 사이의 이진 교차 엔트로피 손실을 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors, Functions and Computational graph\n",
    "  \n",
    "This code defines the following computational graph:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/_static/img/basics/comp-graph.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this network, w and b are **parameters**, which we need to optimize.   \n",
    "\n",
    "Thus, we need to be able to compute the gradients of loss function with respect to those variables.   \n",
    "In order to do that, we set the ``requires_grad`` property of those tensors.\n",
    "\n",
    "PyTorch의 autograd 패키지는 계산 그래프(computation graph)를 활용하여 텐서 연산에 대한 그래디언트를 자동으로 계산  \n",
    "\n",
    "requires_grad=True: 텐서를 생성할 때 이 속성을 True로 설정하면,   \n",
    "해당 텐서의 모든 연산이 추적되며, 계산 그래프에 해당 연산이 기록됨   \n",
    "\n",
    "이 텐서의 .grad 속성을 통해 그래디언트를 얻을 수 있음  \n",
    "이는 주로 모델의 가중치나 학습 가능한 매개변수에 적용됨  \n",
    "\n",
    "requires_grad=False (기본값): 텐서를 생성할 때 이 속성을 False로 설정하면, 해당 텐서의 연산은 추적되지 않음  \n",
    "그래디언트를 계산할 필요가 없는 경우에 사용됨   \n",
    "이 설정을 통해 연산의 속도를 향상시키고 메모리를 절약할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "You can set the value of ``requires_grad`` when creating a tensor,   \n",
    "or later by using ``x.requires_grad_(True)`` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that we apply to tensors to construct computational graph is in fact an object of class Function.   \n",
    "\n",
    "This object knows how to compute the function in the forward direction,   \n",
    "\n",
    "and also how to compute its derivative(도함수) during the backward propagation step. \n",
    "\n",
    "A reference to the backward propagation function is stored in grad_fn property of a tensor.   \n",
    "텐서의 grad_fn 속성에 역전파 함수에 대한 참조가 저장  \n",
    "\n",
    "You can find more information of Function in the documentation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001C55A278970>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001C55A2004C0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients\n",
    "\n",
    "To optimize weights of parameters in the neural network,   \n",
    "\n",
    "we need to compute the derivatives(도함수) of our loss function with respect to parameters,  \n",
    "\n",
    "namely, we need $\\frac{\\partial loss}{\\partial w}$ and\n",
    "$\\frac{\\partial loss}{\\partial b}$ under some fixed values of\n",
    "``x`` and ``y``.   \n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w}$ : 손실 함수를 가중치 w에 대해 편미분  \n",
    "일반적으로 딥러닝에서는 손실 함수를 최소화하는 것이 목표이므로,  \n",
    "가중치 w를 조금 변경했을 때 손실 함수가 얼마나 변화하는지 나타냄  \n",
    "\n",
    "To compute those derivatives, we call\n",
    "``loss.backward()``,   \n",
    "\n",
    "자동 미분(autograd)을 사용하여 그래디언트(기울기)를 계산하고 각 매개변수의 .grad 속성에 저장  \n",
    "\n",
    "and then retrieve the values from ``w.grad`` and\n",
    "``b.grad``:  \n",
    "\n",
    "retrieve : 정보, 데이터, 값 등을 가져오거나 얻어오는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2811, 0.2018, 0.1770],\n",
      "        [0.2811, 0.2018, 0.1770],\n",
      "        [0.2811, 0.2018, 0.1770],\n",
      "        [0.2811, 0.2018, 0.1770],\n",
      "        [0.2811, 0.2018, 0.1770]])\n",
      "tensor([0.2811, 0.2018, 0.1770])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "\n",
    "- We can only obtain(구하다) the grad properties for the leaf nodes of the computational graph,   \n",
    "which have ``requires_grad`` property set to ``True``.   \n",
    "For all other nodes in our graph, gradients will not be available.  \n",
    "\n",
    "- We can only perform gradient calculations using backward once on a given graph, for performance reasons.   \n",
    "주어진 그래프에서는 성능상의 이유로 한 번만 역전파를 수행할 수 있음  \n",
    "(한 번의 역전파 후에는 이 그래프가 해제되고 메모리에서 삭제됨, 이렇게 함으로써 성능을 향상)  \n",
    "If we need to do several backward calls on the same graph,  \n",
    "동일한 그래프에 대해 여러 번 역전파를 수행해야 하는 경우  \n",
    "we need to pass ``retain_graph=True`` to the backward call.  \n",
    "retain_graph=True를 backward 호출에 전달하여 그래프를 유지하도록 할 수 있음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling Gradient Tracking\n",
    "By default, all tensors with ``requires_grad=True`` are tracking their computational history and support gradient computation.   \n",
    "기본적으로 requires_grad=True로 설정된 모든 텐서는 계산 이력을 추적하며 그래디언트 계산을 지원  \n",
    "\n",
    "However, there are some cases when we do not need to do that,   \n",
    "\n",
    "for example, when we have trained the model and just want to apply it to some input data,   \n",
    "모델을 훈련시키고 나서 입력 데이터에 모델을 적용하기만 하려는 경우  \n",
    "\n",
    "i.e. we only want to do forward computations through the network.   \n",
    " 네트워크를 통해 전방 계산만 수행하고 그래디언트 계산이 필요하지 않을 때  \n",
    " \n",
    "We can stop tracking computations by surrounding our computation code with ``torch.no_grad()`` block:  \n",
    "torch.no_grad() 블록으로 계산 코드를 감싸면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to achieve the same result is to use the ``detach()`` method on the tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "  - To mark some parameters in your neural network as **frozen parameters**.\n",
    "\n",
    "  신경망의 일부 매개변수를 frozen parameters로 표시  \n",
    "  모델을 훈련할 때 특정 매개변수의 업데이트를 막아야 하는 경우가 있음  \n",
    "  이를 통해 미리 학습된 가중치를 고정하고 새로운 데이터에 대한 전방 전파만 수행할 수 있음  \n",
    "  \n",
    "  - To **speed up computations** when you are only doing forward pass,   \n",
    "  because computations on tensors that do not track gradients would be more efficient.\n",
    "\n",
    "  전방 전파만 수행할 때 연산 속도를 높이기 위해:   \n",
    "  그래디언트를 추적하지 않는 텐서의 연산은 더 효율적으로 수행됨  \n",
    "  따라서 학습 중이 아닌 단계에서 속도를 높이고 메모리를 절약하기 위해 그래디언트 추적을 비활성화할 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Computational Graphs\n",
    "Conceptually(개념적으로), autograd keeps a record of data (tensors)   \n",
    "and all executed operations (along with the resulting new tensors)   \n",
    "in a directed acyclic graph (DAG) consisting of Function objects.   \n",
    "\n",
    "autograd는 / Function 객체로 이루어진 DAG에서 / 데이터(텐서)와 / 수행된 모든 연산을 기록함\n",
    "  \n",
    "In this DAG, leaves are the input tensors, roots are the output tensors.   \n",
    "By tracing this graph from roots to leaves,  \n",
    "you can automatically compute the gradients using the chain rule.  \n",
    "\n",
    "DAG에서 잎(Leaves)은 입력 텐서이고, 뿌리(Roots)는 출력 텐서  \n",
    "이 그래프를 뿌리에서 잎까지 추적함으로써 연쇄 법칙(chain rule)을 사용하여 자동으로 그래디언트를 계산할 수 있음  \n",
    "\n",
    "In a forward pass, autograd does two things simultaneously(동시에):\n",
    "\n",
    "- run the requested operation to compute a resulting tensor  \n",
    "요청된 연산을 실행하여 결과 텐서를 계산\n",
    "- maintain the operation’s *gradient function* in the DAG.  \n",
    "DAG에서 연산의 그래디언트 함수를 유지\n",
    "\n",
    "The backward pass kicks off when ``.backward()`` is called on the DAG\n",
    "root. ``autograd`` then:  \n",
    "역전파는 DAG의 루트에서 .backward()가 호출될 때 시작됨, 그 후 autograd는 :  \n",
    "\n",
    "- computes the gradients from each ``.grad_fn``,\n",
    "- accumulates(누적) them in the respective(각각의) tensor’s ``.grad`` attribute\n",
    "- using the chain rule, propagates all the way to the leaf tensors.  \n",
    "연쇄 법칙을 사용하여 leaf tensors까지 전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE\n",
    "**DAGs are dynamic in PyTorch**   \n",
    "각각의 역전파 단계에서 새로운 그래프가 생성되어 동적인 모델 구성이 가능하게 됨\n",
    "\n",
    "An important thing to note is that the graph is recreated from scratch;   \n",
    "그래프가 매번 새로 생성  \n",
    "from scratch : 아무 것도 없는 상태에서 시작한다는 의미  \n",
    "\n",
    "after each .backward() call, autograd starts populating a new graph.   \n",
    "각 .backward() 호출 이후에 autograd는 새로운 그래프를 구성  \n",
    "\n",
    "This is exactly what allows you to use control flow statements in your model;   \n",
    "모델에서 제어 흐름 문장을 사용할 수 있는 것을 정확히 의미  \n",
    "\n",
    "you can change the shape, size and operations at every iteration if needed.  \n",
    "필요한 경우 각 반복에서 모양, 크기 및 연산을 변경할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Reading: Tensor Gradients and Jacobian Products\n",
    "\n",
    "In many cases, we have a scalar loss function,   \n",
    "and we need to compute the gradient with respect to some parameters.  \n",
    " with respect to : ~에 관하여\n",
    "  \n",
    "However, there are cases\n",
    "when the output function is an arbitrary(임의의) tensor.   \n",
    "In this case, PyTorch allows you to compute so-called **Jacobian product**,  \n",
    "and not the actual gradient.\n",
    "\n",
    "For a vector function $\\vec{y}=f(\\vec{x})$, where\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and\n",
    "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$,   \n",
    "\n",
    "벡터 함수 $\\vec{y}=f(\\vec{x})$가 있을 때,\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$이고\n",
    "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$이라면,  \n",
    "\n",
    "\n",
    "a gradient of $\\vec{y}$ with respect to $\\vec{x}$ is given by **Jacobian\n",
    "matrix**:\n",
    "\n",
    "$\\vec{y}$에 대한 $\\vec{x}$의 그래디언트  \n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Instead of computing the Jacobian matrix itself,   \n",
    "Jacobian 행렬 자체를 계산하는 대신,   \n",
    "\n",
    "PyTorch allows you to compute **Jacobian Product** $v^T\\cdot J$ for a given input vector\n",
    "$v=(v_1 \\dots v_m)$.   \n",
    "주어진 입력 벡터 $v=(v_1 \\dots v_m)$에 대한 Jacobian Product $v^T\\cdot J$를 계산할 수 있음  \n",
    "\n",
    "This is achieved by calling ``backward`` with $v$ as an argument.   \n",
    "이는 backward를 $v$와 함께 호출함으로써 달성  \n",
    "\n",
    "The size of $v$ should be the same as the size of the original tensor,   \n",
    "$v$의 크기는 원래 텐서와 동일해야 하며,  \n",
    "\n",
    "with respect to which we want to compute the product:  \n",
    "계산하려는 그래디언트의 원래 텐서에 대한 크기와 일치해야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "# torch.eye(4, 5, requires_grad=True) :  4x5 크기의 단위 행렬 생성\n",
    "# 대각 원소가 1이고 나머지는 0인 행렬\n",
    "\n",
    "out = (inp+1).pow(2).t()\n",
    "# inp에 1을 더한 후 제곱, transpose\n",
    "\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "# out에 대한 그래디언트 계산\n",
    "# torch.ones_like(out) : out과 같은 크기의 1로 이루어진 텐서를 생성\n",
    "# 이를 사용하여 out.backward를 호출하여 그래디언트 계산\n",
    "# retain_graph=True : 그래프를 유지하도록 지정\n",
    "\n",
    "print(f\"First call\\n{inp.grad}\")  # 그래디언트 출력\n",
    "\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "# 그래프를 다시 사용하여 out에 대한 그래디언트 계산\n",
    "\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "# 다시 그래디언트를 출력, 이전에 계산된 그래디언트에 현재 계산된 그래디언트가 더해지게 됨\n",
    "\n",
    "inp.grad.zero_() # inp.grad를 0으로 초기화\n",
    "\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "# 그래프를 다시 사용하여 out에 대한 그래디언트를 계산\n",
    "# inp.grad는 이전에 초기화되었기 때문에 현재 계산된 그래디언트로 덮어씌워짐\n",
    "\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")\n",
    "# 마지막으로 그래디언트 출력, 초기화 후에 계산된 그래디언트임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we call ``backward`` for the second time with the same\n",
    "argument,   \n",
    "the value of the gradient is different.   \n",
    "동일한 인자로/ 두번째로 backward를 호출할 때 그래디언트 값이 다르다는 점에 주목  \n",
    "\n",
    "This happens because when doing ``backward`` propagation, PyTorch **accumulates the\n",
    "gradients**, i.e.   \n",
    "backward 역전파를 수행할 때 PyTorch가 그래디언트를 **누적(accumulate)**하기 때문  \n",
    "\n",
    "the value of computed gradients is added to the\n",
    "``grad`` property of all leaf nodes of computational graph.    \n",
    "즉, 계산된 그래디언트의 값이 계산 그래프의 모든 leaf nodes의 grad 속성에 추가됨  \n",
    "\n",
    "If you want to compute the proper gradients, you need to zero out the ``grad`` property before.   \n",
    "올바른 그래디언트를 계산하려면 backward를 호출하기 전에 grad 속성을 0으로 초기화해야 함  \n",
    "\n",
    "In real-life training an *optimizer* helps us to do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE\n",
    "Previously we were calling backward() function without parameters.   \n",
    "This is essentially equivalent to calling backward(torch.tensor(1.0)),   \n",
    "\n",
    "매개변수 없이 backward() 함수 호출은  본질적으로 backward(torch.tensor(1.0))을 호출하는 것과 동등  \n",
    "\n",
    "which is a useful way to compute the gradients in case of a scalar-valued function,   \n",
    "손실 함수와 같이 스칼라 값 함수에 대한 그래디언트를 계산하는 유용한 방법    \n",
    "\n",
    "such as loss during neural network training.  \n",
    "특히 신경망 훈련 중에 손실을 다룰 때 사용됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

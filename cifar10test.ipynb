{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets,transforms, models\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install torch==2.1.2+cu121 torchvision==0.16.2+cu121 torchaudio==2.1.2+cu121 -f https://download.pytorch.org/whl/cu121/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "현재 작업 디렉토리 : c:\\Users\\Tzuyu\\Desktop\\PythonWorkspace\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 1660 SUPER\n",
      "cuda version : 12.1\n",
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "print(\"현재 작업 디렉토리 :\", os.getcwd())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())  # 1 이상의 숫자가 출력되어야 함\n",
    "print(torch.cuda.get_device_name(0))  # GPU 이름\n",
    "print(\"cuda version :\", torch.version.cuda)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 정의\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'cifar10/train'\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame({'img_id': [f'{i:05d}' for i in range(50000)]}) # train data 50000개\n",
    "df['label_path'] = df['img_id'].apply(lambda x: os.path.join(root_dir, f'{x}.txt'))\n",
    "\n",
    "# 레이블 파일에서 레이블을 읽어와 데이터프레임에 추가\n",
    "df['label'] = df['label_path'].apply(lambda path: int(open(path, 'r').readline().strip()) if os.path.exists(path) else None)\n",
    "\n",
    "# 레이블이 있는 데이터만 사용\n",
    "df = df.dropna()\n",
    "\n",
    "# 좌우 반전 및 새로운 파일명으로 저장 (50000~99999)\n",
    "for idx in range(len(df)):\n",
    "    img_id = df.iloc[idx]['img_id']\n",
    "    img_path = os.path.join(root_dir, f'{img_id}.png')  # 이미지 파일 경로\n",
    "    label = df.iloc[idx]['label']  # 해당 이미지에 대한 레이블\n",
    "\n",
    "    # 이미지 불러오기\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # 좌우 반전\n",
    "    flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    # 새로운 파일명 생성\n",
    "    new_img_id = f'{idx + 50000:05d}' # 원본 인덱스 + 50000 # 0을 뒤집은 이미지 -> 50000 # 21456을 뒤집은 이미지 -> 71456\n",
    "    new_img_path = os.path.join(root_dir, f'{new_img_id}.png') # root_dir = 'cifar10/train'\n",
    "    new_label_path = os.path.join(root_dir, f'{new_img_id}.txt')\n",
    "\n",
    "    # 저장\n",
    "    flipped_image.save(new_img_path)\n",
    "    with open(new_label_path, 'w') as label_file:\n",
    "        label_file.write(str(label))  # 해당 이미지의 원본 레이블 값을 새로운 레이블 파일에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 제대로 실행되었는지 확인\n",
    "import matplotlib.pyplot as plt\n",
    "# 이미지 불러오기\n",
    "img_id_0 = '00001'\n",
    "img_path_0 = os.path.join(root_dir, f'{img_id_0}.png')\n",
    "label_0 = int(open(os.path.join(root_dir, f'{img_id_0}.txt'), 'r').readline().strip())\n",
    "image_0 = Image.open(img_path_0).convert('RGB')\n",
    "\n",
    "img_id_50000 = '50001' # 00001 + 50000\n",
    "img_path_50000 = os.path.join(root_dir, f'{img_id_50000}.png')\n",
    "label_50000 = int(open(os.path.join(root_dir, f'{img_id_50000}.txt'), 'r').readline().strip())\n",
    "image_50000 = Image.open(img_path_50000).convert('RGB')\n",
    "\n",
    "# 이미지와 레이블 표시\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(image_0)\n",
    "axes[0].set_title(f\"Label: {label_0}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(image_50000)\n",
    "axes[1].set_title(f\"Label: {label_50000}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetCombined(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if self.root_dir == 'cifar10/train':\n",
    "            range_num = 100000\n",
    "        elif self.root_dir == 'cifar10/test':\n",
    "            range_num = 10000\n",
    "\n",
    "        df = pd.DataFrame({'img_id': [f'{i:05d}' for i in range(range_num)]})\n",
    "        df['label_path'] = df['img_id'].apply(lambda x: os.path.join(root_dir, f'{x}.txt'))\n",
    "        \n",
    "        # 레이블 파일에서 레이블을 읽어와 데이터프레임에 추가\n",
    "        df['label'] = df['label_path'].apply(lambda path: int(open(path, 'r').readline().strip()))\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx]['img_id']\n",
    "        img_path = os.path.join(self.root_dir, f'{img_id}.png')  # 이미지 파일 경로       \n",
    "        label = self.df.iloc[idx]['label']  # 해당 이미지에 대한 레이블\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 100000\n",
      "Number of test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# 큰 배치 크기는 한 번에 더 많은 데이터를 처리 - 계산 속도 상승 가능, 메모리 많이 필요\n",
    "# 작은 배치 크기는 모델이 각 배치에 노출되는 데이터가 다양하게 - 모델 더 빠르게 수렴, 더 나은 일반화 성능 가능\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# torchvision 데이터셋의 출력(output)은 [0, 1] 범위를 갖는 PILImage 이미지\n",
    "# 이를 [-1, 1]의 범위로 정규화된 Tensor로 변환 (공식 문서)\n",
    "\n",
    "aug_dataset = CustomDatasetCombined(root_dir='cifar10/train', subset='train', transform=transform)\n",
    "aug_dataloader = torch.utils.data.DataLoader(aug_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "print(f\"Number of training samples: {len(aug_dataset)}\")\n",
    "\n",
    "test_dataset = CustomDatasetCombined(root_dir='cifar10/test',  subset='test', transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# __init__은 모델의 구조를 초기화\n",
    "# forward는 입력 데이터를 받아 모델을 통과시켜 예측값을 계산\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, padding_mode='replicate')  \n",
    "        # 입력 채널 3, 출력 채널 32, 커널 크기 3, 패딩 1\n",
    "        # 입력 채널 3 : 컬러 이미지 - 빨강, 초록, 파랑 (RGB) 세 가지 채널\n",
    "        # 출력 채널 32 : 특징 맵의 수  \n",
    "        # 커널 크기 3 : 각 픽셀에 대해 3x3 크기의 커널 사용\n",
    "        # 패딩 1 : 커널 크기가 3x3이므로 자연스럽게 패딩 1 / 커널 크기 5x5라면 패딩 2\n",
    "        # 패딩 파라미터를 입력하지 않으면 기본값으로 0이 사용, 출력이 줄어들게 됨\n",
    "        # Replicate-padding: 가장자리에 있는 픽셀을 복사하여 패딩을 채움\n",
    "        \n",
    "        # 기본 패딩 (제로패딩) 정확도 : % (100에포크, adam)\n",
    "        # replicate 패딩 정확도      : % (100에포크, adam)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm2d(32) \n",
    "        # 32 : 배치 정규화 레이어에 입력되는 채널의 수\n",
    "        # 32개의 채널 각각에 대해 배치 정규화 수행\n",
    "        # 각 채널이 독립적으로 정규화되어 네트워크가 더 안정적으로 학습되도록 도와주는 효과\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        # 2x2 크기의 풀링 윈도우, 보폭 2 - 입력 데이터를 2픽셀씩 이동하면서 최댓값을 추출\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        # 25%의 노드가 랜덤하게 비활성화\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 512)\n",
    "        # 64 * 6 * 6 : 앞선 레이어를 거치면서 얻은 출력 특징 맵의 크기 \n",
    "        # 512: 이 fully connected 레이어의 출력 크기, 512개의 노드로 연결\n",
    "        \n",
    "        self.batch_norm5 = nn.BatchNorm1d(512)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.batch_norm3(self.conv3(x)))\n",
    "        x = F.relu(self.batch_norm4(self.conv4(x)))\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.batch_norm5(self.fc1(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (batch_norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batch_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_norm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batch_norm4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout2): Dropout(p=0.25, inplace=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=2304, out_features=512, bias=True)\n",
      "  (batch_norm5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gmnet = CustomModel(num_classes=10).to(device)\n",
    "\n",
    "# 손실 함수 및 optimizer 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.SGD(gmnet.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = optim.RMSprop(gmnet.parameters(), lr=0.001, alpha=0.9, weight_decay=0.01)\n",
    "optimizer = optim.Adam(gmnet.parameters(), lr=0.001)\n",
    "\n",
    "print(gmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 함수 정의\n",
    "def train(gmnet, aug_dataloader, criterion, optimizer, device):\n",
    "    gmnet.train() # 모델을 학습 모드로 설정 : 드롭아웃과 같은 학습 중에만 활성화되어야 하는 연산들을 활성화\n",
    "    running_loss = 0.0 # 현재 미니배치까지의 누적 손실을 저장\n",
    "\n",
    "    for inputs, labels in aug_dataloader: # 입력 데이터와 레이블을 가져옴\n",
    "        inputs, labels = inputs.to(device), labels.to(device) #  데이터를 GPU로 이동\n",
    "\n",
    "        optimizer.zero_grad() # 기울기 초기화\n",
    "\n",
    "        # 모델을 통과한 결과를 얻고 손실을 계산\n",
    "        outputs = gmnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # 역전파를 수행하여 기울기를 계산\n",
    "        optimizer.step() # 최적화를 수행하여 가중치를 업데이트\n",
    "\n",
    "        running_loss += loss.item() # 현재 미니배치의 손실을 누적\n",
    "\n",
    "    return running_loss / len(aug_dataloader) # 전체 훈련 데이터에 대한 평균 손실을 반환\n",
    "\n",
    "# 테스트 함수 정의\n",
    "def test(gmnet, test_dataloader, criterion, device):\n",
    "    gmnet.eval()            # 모델을 평가 모드로 설정\n",
    "    correct_predictions = 0 # 올바르게 예측된 총 샘플 수를 저장\n",
    "    total_samples = 0       # 총 테스트 샘플 수를 저장\n",
    "\n",
    "    # 그래디언트 계산을 비활성화하고 테스트 데이터로부터 입력 데이터와 레이블을 가져옴\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 모델을 통과한 결과를 얻고, 가장 높은 확률을 가진 클래스로 예측\n",
    "            # 정확한 예측 수와 전체 샘플 수를 업데이트\n",
    "            outputs = gmnet(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "            # torch.max(outputs, 1) : 모델의 출력에서 각 행마다 최대값과 해당 최대값의 인덱스를 반환\n",
    "            # _ : 최대값, predicted : 각 입력 샘플에 대한 예측된 클래스의 인덱스를 나타냄\n",
    "            \n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            # 예측된 클래스와 실제 레이블 labels를 비교하여 올바르게 예측된 샘플 수를 누적\n",
    "            # (predicted == labels) : 각 위치에서 예측이 맞으면 True, 틀리면 False를 가지는 텐서를 생성\n",
    "            # sum().item() : True의 개수를 합산하여 올바르게 예측된 전체 수를 얻음\n",
    "            \n",
    "            total_samples += labels.size(0) \n",
    "            # 테스트된 전체 샘플 수를 업데이트\n",
    "            # labels.size(0) : 현재 미니배치의 샘플 수\n",
    "\n",
    "    # 전체 테스트 데이터에 대한 정확도를 계산\n",
    "    accuracy = correct_predictions / total_samples * 100 \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# BasicBlock 정의\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    " # ResNet 정의\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.dropout3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = F.softmax(out, dim=1)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck 블록 정의\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정된 ResNet 클래스 정의\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 모델 인스턴스 생성\n",
    "resnet50 = ResNet50(Bottleneck, [3, 4, 6, 3], num_classes=10).to(device)\n",
    "\n",
    "# 손실 함수 및 최적화기 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(resnet50.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = optim.RMSprop(resnet50.parameters(), lr=0.001, alpha=0.9, weight_decay=0.01)\n",
    "optimizer = optim.Adam(resnet50.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 함수 정의\n",
    "def train(resnet50, aug_dataloader, criterion, optimizer, device):\n",
    "    resnet50.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in aug_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet50(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(aug_dataloader)\n",
    "\n",
    "# 테스트 함수 정의\n",
    "def test(resnet50, test_dataloader, criterion, device):\n",
    "    resnet50.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = resnet50(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 1.9157, Test Accuracy: 66.64%\n",
      "Epoch [2/200], Train Loss: 1.8042, Test Accuracy: 71.78%\n",
      "Epoch [3/200], Train Loss: 1.7714, Test Accuracy: 73.64%\n",
      "Epoch [4/200], Train Loss: 1.7490, Test Accuracy: 74.86%\n",
      "Epoch [5/200], Train Loss: 1.7315, Test Accuracy: 76.50%\n",
      "Epoch [6/200], Train Loss: 1.7197, Test Accuracy: 78.01%\n",
      "Epoch [7/200], Train Loss: 1.7112, Test Accuracy: 78.62%\n",
      "Epoch [8/200], Train Loss: 1.7019, Test Accuracy: 79.13%\n",
      "Epoch [9/200], Train Loss: 1.6929, Test Accuracy: 79.96%\n",
      "Epoch [10/200], Train Loss: 1.6901, Test Accuracy: 79.24%\n",
      "Epoch [11/200], Train Loss: 1.6828, Test Accuracy: 80.66%\n",
      "Epoch [12/200], Train Loss: 1.6786, Test Accuracy: 81.04%\n",
      "Epoch [13/200], Train Loss: 1.6732, Test Accuracy: 80.55%\n",
      "Epoch [14/200], Train Loss: 1.6692, Test Accuracy: 81.37%\n",
      "Epoch [15/200], Train Loss: 1.6668, Test Accuracy: 81.04%\n",
      "Epoch [16/200], Train Loss: 1.6625, Test Accuracy: 81.51%\n",
      "Epoch [17/200], Train Loss: 1.6605, Test Accuracy: 81.83%\n",
      "Epoch [18/200], Train Loss: 1.6565, Test Accuracy: 81.71%\n",
      "Epoch [19/200], Train Loss: 1.6568, Test Accuracy: 82.57%\n",
      "Epoch [20/200], Train Loss: 1.6517, Test Accuracy: 82.06%\n",
      "Epoch [21/200], Train Loss: 1.6510, Test Accuracy: 82.55%\n",
      "Epoch [22/200], Train Loss: 1.6469, Test Accuracy: 82.60%\n",
      "Epoch [23/200], Train Loss: 1.6453, Test Accuracy: 83.05%\n",
      "Epoch [24/200], Train Loss: 1.6447, Test Accuracy: 83.19%\n",
      "Epoch [25/200], Train Loss: 1.6425, Test Accuracy: 82.89%\n",
      "Epoch [26/200], Train Loss: 1.6391, Test Accuracy: 82.91%\n",
      "Epoch [27/200], Train Loss: 1.6374, Test Accuracy: 83.87%\n",
      "Epoch [28/200], Train Loss: 1.6351, Test Accuracy: 83.26%\n",
      "Epoch [29/200], Train Loss: 1.6326, Test Accuracy: 83.76%\n",
      "Epoch [30/200], Train Loss: 1.6337, Test Accuracy: 84.23%\n",
      "Epoch [31/200], Train Loss: 1.6309, Test Accuracy: 84.08%\n",
      "Epoch [32/200], Train Loss: 1.6268, Test Accuracy: 83.58%\n",
      "Epoch [33/200], Train Loss: 1.6269, Test Accuracy: 84.43%\n",
      "Epoch [34/200], Train Loss: 1.6274, Test Accuracy: 84.13%\n",
      "Epoch [35/200], Train Loss: 1.6245, Test Accuracy: 84.29%\n",
      "Epoch [36/200], Train Loss: 1.6239, Test Accuracy: 84.19%\n",
      "Epoch [37/200], Train Loss: 1.6212, Test Accuracy: 84.18%\n",
      "Epoch [38/200], Train Loss: 1.6211, Test Accuracy: 84.07%\n",
      "Epoch [39/200], Train Loss: 1.6215, Test Accuracy: 84.29%\n",
      "Epoch [40/200], Train Loss: 1.6195, Test Accuracy: 84.50%\n",
      "Epoch [41/200], Train Loss: 1.6193, Test Accuracy: 84.67%\n",
      "Epoch [42/200], Train Loss: 1.6158, Test Accuracy: 84.39%\n",
      "Epoch [43/200], Train Loss: 1.6165, Test Accuracy: 84.33%\n",
      "Epoch [44/200], Train Loss: 1.6157, Test Accuracy: 84.14%\n",
      "Epoch [45/200], Train Loss: 1.6140, Test Accuracy: 84.95%\n",
      "Epoch [46/200], Train Loss: 1.6137, Test Accuracy: 84.92%\n",
      "Epoch [47/200], Train Loss: 1.6115, Test Accuracy: 84.71%\n",
      "Epoch [48/200], Train Loss: 1.6117, Test Accuracy: 84.88%\n",
      "Epoch [49/200], Train Loss: 1.6119, Test Accuracy: 85.19%\n",
      "Epoch [50/200], Train Loss: 1.6087, Test Accuracy: 85.14%\n",
      "Epoch [51/200], Train Loss: 1.6107, Test Accuracy: 84.82%\n",
      "Epoch [52/200], Train Loss: 1.6081, Test Accuracy: 84.87%\n",
      "Epoch [53/200], Train Loss: 1.6067, Test Accuracy: 85.20%\n",
      "Epoch [54/200], Train Loss: 1.6061, Test Accuracy: 85.12%\n",
      "Epoch [55/200], Train Loss: 1.6053, Test Accuracy: 84.85%\n",
      "Epoch [56/200], Train Loss: 1.6063, Test Accuracy: 85.26%\n",
      "Epoch [57/200], Train Loss: 1.6055, Test Accuracy: 84.59%\n",
      "Epoch [58/200], Train Loss: 1.6052, Test Accuracy: 85.15%\n",
      "Epoch [59/200], Train Loss: 1.6040, Test Accuracy: 84.75%\n",
      "Epoch [60/200], Train Loss: 1.6038, Test Accuracy: 85.16%\n",
      "Epoch [61/200], Train Loss: 1.6037, Test Accuracy: 85.22%\n",
      "Epoch [62/200], Train Loss: 1.6027, Test Accuracy: 85.24%\n",
      "Epoch [63/200], Train Loss: 1.6022, Test Accuracy: 85.36%\n",
      "Epoch [64/200], Train Loss: 1.6014, Test Accuracy: 85.45%\n",
      "Epoch [65/200], Train Loss: 1.6000, Test Accuracy: 85.78%\n",
      "Epoch [66/200], Train Loss: 1.6000, Test Accuracy: 85.85%\n",
      "Epoch [67/200], Train Loss: 1.5989, Test Accuracy: 85.52%\n",
      "Epoch [68/200], Train Loss: 1.5991, Test Accuracy: 86.11%\n",
      "Epoch [69/200], Train Loss: 1.5995, Test Accuracy: 85.69%\n",
      "Epoch [70/200], Train Loss: 1.5981, Test Accuracy: 85.50%\n",
      "Epoch [71/200], Train Loss: 1.5983, Test Accuracy: 85.10%\n",
      "Epoch [72/200], Train Loss: 1.5976, Test Accuracy: 85.41%\n",
      "Epoch [73/200], Train Loss: 1.5973, Test Accuracy: 85.60%\n",
      "Epoch [74/200], Train Loss: 1.5952, Test Accuracy: 86.14%\n",
      "Epoch [75/200], Train Loss: 1.5936, Test Accuracy: 85.66%\n",
      "Epoch [76/200], Train Loss: 1.5941, Test Accuracy: 85.87%\n",
      "Epoch [77/200], Train Loss: 1.5938, Test Accuracy: 85.49%\n",
      "Epoch [78/200], Train Loss: 1.5929, Test Accuracy: 85.78%\n",
      "Epoch [79/200], Train Loss: 1.5918, Test Accuracy: 85.83%\n",
      "Epoch [80/200], Train Loss: 1.5936, Test Accuracy: 85.64%\n",
      "Epoch [81/200], Train Loss: 1.5937, Test Accuracy: 85.81%\n",
      "Epoch [82/200], Train Loss: 1.5922, Test Accuracy: 85.85%\n",
      "Epoch [83/200], Train Loss: 1.5898, Test Accuracy: 85.83%\n",
      "Epoch [84/200], Train Loss: 1.5912, Test Accuracy: 85.83%\n",
      "Epoch [85/200], Train Loss: 1.5905, Test Accuracy: 85.75%\n",
      "Epoch [86/200], Train Loss: 1.5902, Test Accuracy: 86.06%\n",
      "Epoch [87/200], Train Loss: 1.5901, Test Accuracy: 85.71%\n",
      "Epoch [88/200], Train Loss: 1.5899, Test Accuracy: 85.73%\n",
      "Epoch [89/200], Train Loss: 1.5888, Test Accuracy: 85.92%\n",
      "Epoch [90/200], Train Loss: 1.5895, Test Accuracy: 85.80%\n",
      "Epoch [91/200], Train Loss: 1.5890, Test Accuracy: 85.81%\n",
      "Epoch [92/200], Train Loss: 1.5893, Test Accuracy: 85.61%\n",
      "Epoch [93/200], Train Loss: 1.5873, Test Accuracy: 85.91%\n",
      "Epoch [94/200], Train Loss: 1.5863, Test Accuracy: 85.85%\n",
      "Epoch [95/200], Train Loss: 1.5868, Test Accuracy: 86.04%\n",
      "Epoch [96/200], Train Loss: 1.5869, Test Accuracy: 85.99%\n",
      "Epoch [97/200], Train Loss: 1.5853, Test Accuracy: 85.94%\n",
      "Epoch [98/200], Train Loss: 1.5868, Test Accuracy: 85.92%\n",
      "Epoch [99/200], Train Loss: 1.5869, Test Accuracy: 85.60%\n",
      "Epoch [100/200], Train Loss: 1.5867, Test Accuracy: 86.09%\n",
      "Epoch [101/200], Train Loss: 1.5848, Test Accuracy: 86.07%\n",
      "Epoch [102/200], Train Loss: 1.5845, Test Accuracy: 85.89%\n",
      "Epoch [103/200], Train Loss: 1.5848, Test Accuracy: 85.34%\n",
      "Epoch [104/200], Train Loss: 1.5848, Test Accuracy: 86.31%\n",
      "Epoch [105/200], Train Loss: 1.5858, Test Accuracy: 85.90%\n",
      "Epoch [106/200], Train Loss: 1.5836, Test Accuracy: 86.40%\n",
      "Epoch [107/200], Train Loss: 1.5831, Test Accuracy: 86.30%\n",
      "Epoch [108/200], Train Loss: 1.5827, Test Accuracy: 86.12%\n",
      "Epoch [109/200], Train Loss: 1.5832, Test Accuracy: 86.00%\n",
      "Epoch [110/200], Train Loss: 1.5832, Test Accuracy: 86.06%\n",
      "Epoch [111/200], Train Loss: 1.5825, Test Accuracy: 86.54%\n",
      "Epoch [112/200], Train Loss: 1.5822, Test Accuracy: 86.02%\n",
      "Epoch [113/200], Train Loss: 1.5819, Test Accuracy: 85.88%\n",
      "Epoch [114/200], Train Loss: 1.5805, Test Accuracy: 86.34%\n",
      "Epoch [115/200], Train Loss: 1.5818, Test Accuracy: 86.20%\n",
      "Epoch [116/200], Train Loss: 1.5802, Test Accuracy: 86.36%\n",
      "Epoch [117/200], Train Loss: 1.5802, Test Accuracy: 86.26%\n",
      "Epoch [118/200], Train Loss: 1.5808, Test Accuracy: 86.28%\n",
      "Epoch [119/200], Train Loss: 1.5816, Test Accuracy: 86.33%\n",
      "Epoch [120/200], Train Loss: 1.5817, Test Accuracy: 86.29%\n",
      "Epoch [121/200], Train Loss: 1.5789, Test Accuracy: 86.04%\n",
      "Epoch [122/200], Train Loss: 1.5784, Test Accuracy: 86.61%\n",
      "Epoch [123/200], Train Loss: 1.5796, Test Accuracy: 86.21%\n",
      "Epoch [124/200], Train Loss: 1.5787, Test Accuracy: 85.92%\n",
      "Epoch [125/200], Train Loss: 1.5795, Test Accuracy: 86.13%\n",
      "Epoch [126/200], Train Loss: 1.5806, Test Accuracy: 86.37%\n",
      "Epoch [127/200], Train Loss: 1.5789, Test Accuracy: 86.29%\n",
      "Epoch [128/200], Train Loss: 1.5797, Test Accuracy: 86.21%\n",
      "Epoch [129/200], Train Loss: 1.5784, Test Accuracy: 85.83%\n",
      "Epoch [130/200], Train Loss: 1.5792, Test Accuracy: 86.34%\n",
      "Epoch [131/200], Train Loss: 1.5776, Test Accuracy: 85.94%\n",
      "Epoch [132/200], Train Loss: 1.5802, Test Accuracy: 86.33%\n",
      "Epoch [133/200], Train Loss: 1.5772, Test Accuracy: 86.47%\n",
      "Epoch [134/200], Train Loss: 1.5773, Test Accuracy: 86.18%\n",
      "Epoch [135/200], Train Loss: 1.5752, Test Accuracy: 86.10%\n",
      "Epoch [136/200], Train Loss: 1.5771, Test Accuracy: 86.69%\n",
      "Epoch [137/200], Train Loss: 1.5761, Test Accuracy: 86.44%\n",
      "Epoch [138/200], Train Loss: 1.5769, Test Accuracy: 86.58%\n",
      "Epoch [139/200], Train Loss: 1.5761, Test Accuracy: 86.72%\n",
      "Epoch [140/200], Train Loss: 1.5762, Test Accuracy: 86.74%\n",
      "Epoch [141/200], Train Loss: 1.5758, Test Accuracy: 86.29%\n",
      "Epoch [142/200], Train Loss: 1.5757, Test Accuracy: 86.60%\n",
      "Epoch [143/200], Train Loss: 1.5755, Test Accuracy: 86.11%\n",
      "Epoch [144/200], Train Loss: 1.5759, Test Accuracy: 86.50%\n",
      "Epoch [145/200], Train Loss: 1.5751, Test Accuracy: 86.16%\n",
      "Epoch [146/200], Train Loss: 1.5747, Test Accuracy: 85.88%\n",
      "Epoch [147/200], Train Loss: 1.5742, Test Accuracy: 86.08%\n",
      "Epoch [148/200], Train Loss: 1.5757, Test Accuracy: 86.48%\n",
      "Epoch [149/200], Train Loss: 1.5738, Test Accuracy: 86.28%\n",
      "Epoch [150/200], Train Loss: 1.5743, Test Accuracy: 86.05%\n",
      "Epoch [151/200], Train Loss: 1.5732, Test Accuracy: 86.29%\n",
      "Epoch [152/200], Train Loss: 1.5755, Test Accuracy: 86.07%\n",
      "Epoch [153/200], Train Loss: 1.5739, Test Accuracy: 86.56%\n",
      "Epoch [154/200], Train Loss: 1.5712, Test Accuracy: 86.39%\n",
      "Epoch [155/200], Train Loss: 1.5728, Test Accuracy: 85.94%\n",
      "Epoch [156/200], Train Loss: 1.5733, Test Accuracy: 86.53%\n",
      "Epoch [157/200], Train Loss: 1.5725, Test Accuracy: 86.73%\n",
      "Epoch [158/200], Train Loss: 1.5728, Test Accuracy: 86.11%\n",
      "Epoch [159/200], Train Loss: 1.5740, Test Accuracy: 86.18%\n",
      "Epoch [160/200], Train Loss: 1.5716, Test Accuracy: 86.15%\n",
      "Epoch [161/200], Train Loss: 1.5722, Test Accuracy: 86.53%\n",
      "Epoch [162/200], Train Loss: 1.5726, Test Accuracy: 86.37%\n",
      "Epoch [163/200], Train Loss: 1.5739, Test Accuracy: 86.16%\n",
      "Epoch [164/200], Train Loss: 1.5711, Test Accuracy: 86.25%\n",
      "Epoch [165/200], Train Loss: 1.5718, Test Accuracy: 86.28%\n",
      "Epoch [166/200], Train Loss: 1.5722, Test Accuracy: 86.27%\n",
      "Epoch [167/200], Train Loss: 1.5711, Test Accuracy: 86.32%\n",
      "Epoch [168/200], Train Loss: 1.5703, Test Accuracy: 86.33%\n",
      "Epoch [169/200], Train Loss: 1.5720, Test Accuracy: 86.15%\n",
      "Epoch [170/200], Train Loss: 1.5710, Test Accuracy: 86.74%\n",
      "Epoch [171/200], Train Loss: 1.5709, Test Accuracy: 86.81%\n",
      "Epoch [172/200], Train Loss: 1.5710, Test Accuracy: 86.20%\n",
      "Epoch [173/200], Train Loss: 1.5698, Test Accuracy: 86.41%\n",
      "Epoch [174/200], Train Loss: 1.5689, Test Accuracy: 86.65%\n",
      "Epoch [175/200], Train Loss: 1.5708, Test Accuracy: 86.90%\n",
      "Epoch [176/200], Train Loss: 1.5691, Test Accuracy: 86.65%\n",
      "Epoch [177/200], Train Loss: 1.5704, Test Accuracy: 86.46%\n",
      "Epoch [178/200], Train Loss: 1.5693, Test Accuracy: 86.78%\n",
      "Epoch [179/200], Train Loss: 1.5687, Test Accuracy: 86.48%\n",
      "Epoch [180/200], Train Loss: 1.5686, Test Accuracy: 86.39%\n",
      "Epoch [181/200], Train Loss: 1.5705, Test Accuracy: 86.53%\n",
      "Epoch [182/200], Train Loss: 1.5689, Test Accuracy: 86.96%\n",
      "Epoch [183/200], Train Loss: 1.5670, Test Accuracy: 86.81%\n",
      "Epoch [184/200], Train Loss: 1.5684, Test Accuracy: 86.80%\n",
      "Epoch [185/200], Train Loss: 1.5672, Test Accuracy: 86.82%\n",
      "Epoch [186/200], Train Loss: 1.5684, Test Accuracy: 86.53%\n",
      "Epoch [187/200], Train Loss: 1.5695, Test Accuracy: 86.64%\n",
      "Epoch [188/200], Train Loss: 1.5677, Test Accuracy: 86.86%\n",
      "Epoch [189/200], Train Loss: 1.5669, Test Accuracy: 86.64%\n",
      "Epoch [190/200], Train Loss: 1.5675, Test Accuracy: 86.71%\n",
      "Epoch [191/200], Train Loss: 1.5696, Test Accuracy: 86.80%\n",
      "Epoch [192/200], Train Loss: 1.5682, Test Accuracy: 86.87%\n",
      "Epoch [193/200], Train Loss: 1.5668, Test Accuracy: 87.26%\n",
      "Epoch [194/200], Train Loss: 1.5669, Test Accuracy: 86.32%\n",
      "Epoch [195/200], Train Loss: 1.5678, Test Accuracy: 86.39%\n",
      "Epoch [196/200], Train Loss: 1.5686, Test Accuracy: 86.74%\n",
      "Epoch [197/200], Train Loss: 1.5678, Test Accuracy: 86.89%\n",
      "Epoch [198/200], Train Loss: 1.5672, Test Accuracy: 86.84%\n",
      "Epoch [199/200], Train Loss: 1.5676, Test Accuracy: 86.86%\n",
      "Epoch [200/200], Train Loss: 1.5672, Test Accuracy: 87.04%\n",
      "Final Test Accuracy: 87.04%\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 테스트\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(gmnet, aug_dataloader, criterion, optimizer, device)\n",
    "    test_accuracy = test(gmnet, test_dataloader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# 최종 모델의 정확도 출력\n",
    "final_accuracy = test(gmnet, test_dataloader, criterion, device)\n",
    "print(f'Final Test Accuracy: {final_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87 %\n",
      "Accuracy for class: plane is 88.6 %\n",
      "Accuracy for class: car   is 94.6 %\n",
      "Accuracy for class: bird  is 78.6 %\n",
      "Accuracy for class: cat   is 74.3 %\n",
      "Accuracy for class: deer  is 85.6 %\n",
      "Accuracy for class: dog   is 81.8 %\n",
      "Accuracy for class: frog  is 91.7 %\n",
      "Accuracy for class: horse is 91.9 %\n",
      "Accuracy for class: ship  is 91.6 %\n",
      "Accuracy for class: truck is 91.7 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없음\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산\n",
    "        outputs = gmnet(images)\n",
    "        \n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않음\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = gmnet(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모음\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetCombined_2(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        '''\n",
    "        # 데이터프레임 생성\n",
    "        if self.root_dir == 'cifar10/train':\n",
    "            df = pd.DataFrame({'img_id': [f'{i:05d}' for i in range(100000)]})\n",
    "        elif self.root_dir == 'cifar10/test':\n",
    "            df = pd.DataFrame({'img_id': [f'{i:05d}' for i in range(10000)]})\n",
    "        '''\n",
    "        if self.root_dir == 'cifar10/train':\n",
    "            range_num = 100000\n",
    "        elif self.root_dir == 'cifar10/test':\n",
    "            range_num = 10000\n",
    "\n",
    "        df = pd.DataFrame({'img_id': [f'{i:05d}' for i in range(range_num)]})\n",
    "        df['label_path'] = df['img_id'].apply(lambda x: os.path.join(root_dir, f'{x}.txt'))\n",
    "        \n",
    "        # 레이블 파일에서 레이블을 읽어와 데이터프레임에 추가\n",
    "        df['label'] = df['label_path'].apply(lambda path: int(open(path, 'r').readline().strip()))\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx]['img_id']\n",
    "        img_path = os.path.join(self.root_dir, f'{img_id}.png')  # 이미지 파일 경로       \n",
    "        label = self.df.iloc[idx]['label']  # 해당 이미지에 대한 레이블\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 100000\n",
      "Number of test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# 큰 배치 크기는 한 번에 더 많은 데이터를 처리 - 계산 속도 상승 가능, 메모리 많이 필요\n",
    "# 작은 배치 크기는 모델이 각 배치에 노출되는 데이터가 다양하게 - 모델 더 빠르게 수렴, 더 나은 일반화 성능 가능\n",
    "\n",
    "def random_crop_with_replicate_padding(img, size):\n",
    "    # Replicate Padding\n",
    "    img_array = np.array(img) # 입력 이미지를 numpy 배열로 변환\n",
    "    \n",
    "    img_padded = np.pad(img_array, ((4, 4), (4, 4), (0, 0)), mode='edge')\n",
    "    # 첫 번째 차원(높이)에 위아래로 각각 4픽셀 / 두 번째 차원(너비)에 좌우로 각각 4픽셀 / 세 번째 차원(채널)에는 패딩을 추가 X\n",
    "    # mode='edge': 패딩을 할 때 가장자리의 값을 반복하여 사용하는 모드, 'edge' 모드에서는 가장자리 값을 반복하여 패딩\n",
    "    \n",
    "    img_padded = Image.fromarray(img_padded) # 패딩된 numpy 배열을 다시 PIL 이미지로 변환\n",
    "    \n",
    "    # Random Crop\n",
    "    # 패딩된 이미지에서 크롭할 영역의 시작점 (i, j)와  크기 (h, w)를 반환\n",
    "    i, j, h, w = transforms.RandomCrop.get_params(img_padded, output_size=size)\n",
    "    \n",
    "    img_cropped = transforms.functional.crop(img_padded, i, j, h, w)\n",
    "    # transforms.functional.crop : 이미지에서 주어진 좌표 (i, j)와 크기 (h, w)로 정의된 영역을 자르는 함수\n",
    "    # crop(img: PIL.Image.Image, top: int, left: int, height: int, width: int)\n",
    "    # i: 크롭을 적용할 영역의 top 좌표 # j: 크롭을 적용할 영역의 left 좌표\n",
    "    # h: 크롭할 영역의 높이 (height) # w: 크롭할 영역의 너비 (width)\n",
    "\n",
    "    return img_cropped\n",
    "\n",
    "######### train\n",
    "\n",
    "transform_crop = transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: random_crop_with_replicate_padding(x, size=(32, 32))),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "aug_dataset_2 = CustomDatasetCombined_2(root_dir='cifar10/train', transform=transform_crop)\n",
    "aug_dataloader_2 = torch.utils.data.DataLoader(aug_dataset_2, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "print(f\"Number of training samples: {len(aug_dataset_2)}\")\n",
    "\n",
    "######### test\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# torchvision 데이터셋의 출력(output)은 [0, 1] 범위를 갖는 PILImage 이미지\n",
    "# 이를 [-1, 1]의 범위로 정규화된 Tensor로 변환 (공식 문서)\n",
    "\n",
    "test_dataset_2 = CustomDatasetCombined_2(root_dir='cifar10/test', transform=transform)\n",
    "test_dataloader_2 = torch.utils.data.DataLoader(test_dataset_2, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(f\"Number of test samples: {len(test_dataset_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (batch_norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batch_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_norm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batch_norm4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout2): Dropout(p=0.25, inplace=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=2304, out_features=512, bias=True)\n",
      "  (batch_norm5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gmnet_2 = CustomModel(num_classes=10).to(device)\n",
    "\n",
    "# 손실 함수 및 optimizer 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = optim.SGD(gmnet.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = optim.RMSprop(gmnet.parameters(), lr=0.001, alpha=0.9, weight_decay=0.01)\n",
    "optimizer = optim.Adam(gmnet_2.parameters(), lr=0.001)\n",
    "\n",
    "print(gmnet_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 함수 정의\n",
    "def train(gmnet_2, aug_dataloader_2, criterion, optimizer, device):\n",
    "    gmnet_2.train() # 모델을 학습 모드로 설정 : 드롭아웃과 같은 학습 중에만 활성화되어야 하는 연산들을 활성화\n",
    "    running_loss = 0.0 # 현재 미니배치까지의 누적 손실을 저장\n",
    "\n",
    "    for inputs, labels in aug_dataloader_2: # 입력 데이터와 레이블을 가져옴\n",
    "        inputs, labels = inputs.to(device), labels.to(device) #  데이터를 GPU로 이동\n",
    "\n",
    "        optimizer.zero_grad() # 기울기 초기화\n",
    "\n",
    "        # 모델을 통과한 결과를 얻고 손실을 계산\n",
    "        outputs = gmnet_2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # 역전파를 수행하여 기울기를 계산\n",
    "        optimizer.step() # 최적화를 수행하여 가중치를 업데이트\n",
    "\n",
    "        running_loss += loss.item() # 현재 미니배치의 손실을 누적\n",
    "\n",
    "    return running_loss / len(aug_dataloader_2) # 전체 훈련 데이터에 대한 평균 손실을 반환\n",
    "\n",
    "# 테스트 함수 정의\n",
    "def test(gmnet_2, test_dataloader_2, criterion, device):\n",
    "    gmnet_2.eval()            # 모델을 평가 모드로 설정\n",
    "    correct_predictions = 0 # 올바르게 예측된 총 샘플 수를 저장\n",
    "    total_samples = 0       # 총 테스트 샘플 수를 저장\n",
    "\n",
    "    # 그래디언트 계산을 비활성화하고 테스트 데이터로부터 입력 데이터와 레이블을 가져옴\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader_2:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 모델을 통과한 결과를 얻고, 가장 높은 확률을 가진 클래스로 예측\n",
    "            # 정확한 예측 수와 전체 샘플 수를 업데이트\n",
    "            outputs = gmnet_2(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "            # torch.max(outputs, 1) : 모델의 출력에서 각 행마다 최대값과 해당 최대값의 인덱스를 반환\n",
    "            # _ : 최대값, predicted : 각 입력 샘플에 대한 예측된 클래스의 인덱스를 나타냄\n",
    "            \n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            # 예측된 클래스와 실제 레이블 labels를 비교하여 올바르게 예측된 샘플 수를 누적\n",
    "            # (predicted == labels) : 각 위치에서 예측이 맞으면 True, 틀리면 False를 가지는 텐서를 생성\n",
    "            # sum().item() : True의 개수를 합산하여 올바르게 예측된 전체 수를 얻음\n",
    "            \n",
    "            total_samples += labels.size(0) \n",
    "            # 테스트된 전체 샘플 수를 업데이트\n",
    "            # labels.size(0) : 현재 미니배치의 샘플 수\n",
    "\n",
    "    # 전체 테스트 데이터에 대한 정확도를 계산\n",
    "    accuracy = correct_predictions / total_samples * 100 \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(gmnet_2, aug_dataloader_2, criterion, optimizer, device)\n\u001b[0;32m      6\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m test(gmnet_2, test_dataloader_2, criterion, device)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# 훈련 및 테스트\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(gmnet_2, aug_dataloader_2, criterion, optimizer, device)\n",
    "    test_accuracy = test(gmnet_2, test_dataloader_2, criterion, device)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# 최종 모델의 정확도 출력\n",
    "final_accuracy = test(gmnet_2, test_dataloader_2, criterion, device)\n",
    "print(f'Final Test Accuracy: {final_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices.  \n",
    "텐서는 arrays 및 matrices와 매우 유사한 전문 데이터 구조  \n",
    "내부에 데이터를 배열 형태로 저장   \n",
    "배열을 잘 처리하기 위한 하나의 클래스 구조임  \n",
    "배치로 나누고, 자동미분 적용을 위한 처리 등 멤버함수가 구현되어있음  \n",
    "\n",
    "In PyTorch, we use tensors / to encode the inputs and outputs of a model, as well as the model’s parameters.  \n",
    "PyTorch에서는 텐서를 사용하여 / 모델의 입력과 출력 : 모델의 파라미터를 인코딩  \n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators.   \n",
    "텐서는 GPU나 다른 하드웨어 가속기에서 실행할 수 있다는 점을 제외하고는 NumPy의 nandarray와 유사  \n",
    "\n",
    "In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see Bridge with NumPy).   \n",
    "실제로 텐서와 NumPy 어레이는 동일한 기본 메모리를 공유할 수 있으므로 데이터를 복사할 필요가 없음(Bridge with NumPy 참조)\n",
    "\n",
    "Tensors are also optimized for automatic differentiation (we’ll see more about that later in the Autograd section).  \n",
    "또한 텐서는 자동 차별화에 최적화되어 있음(이에 대한 자세한 내용은 나중에 Autograd 섹션에서 확인)  \n",
    "\n",
    "If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!  \n",
    "ndarray에 익숙하다면 Tensor API를 집에서 바로 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Tensor\n",
    "텐서 초기화  \n",
    "\n",
    "ensors can be initialized in various ways. Take a look at the following examples:  \n",
    ": 텐서는 다양한 방법으로 초기화할 수 있음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Directly from data**  \n",
    "\n",
    "Tensors can be created directly from data.   \n",
    "텐서는 데이터에서 직접 만들 수 있음\n",
    "\n",
    "The data type is automatically inferred.  \n",
    "데이터 유형은 자동으로 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]] \n",
    "x_data = torch.tensor(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa - see `bridge-to-np-label`)   \n",
    "텐서는 NumPy 어레이에서 생성할 수 있음(그 반대의 경우도 마찬가지)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data) \n",
    "x_np = torch.from_numpy(np_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **From another tensor:**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.  \n",
    "새로운 텐서는 명시적으로 무시되지 않는 한 인수 텐서의 속성(형상, 데이터 유형)을 유지  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.8398, 0.4272],\n",
      "        [0.0675, 0.6215]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains유지하다 the properties of x_data \n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\") \n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data \n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **With random or constant values:**\n",
    "\n",
    "``shape`` is a tuple of tensor dimensions.   \n",
    "shape 는 텐서 차원의 튜플  \n",
    "\n",
    "In the functions below, it determines the dimensionality of the output tensor.  \n",
    "아래 함수에서 출력 텐서의 치수를 결정  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.8822, 0.3106, 0.7585],\n",
      "        [0.3490, 0.9206, 0.9720]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,) # 마지막 ,는 있어도 되고 없어도 됨\n",
    "rand_tensor = torch.rand(shape) \n",
    "ones_tensor = torch.ones(shape) \n",
    "zeros_tensor = torch.zeros(shape)  \n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\") \n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\") \n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of a Tensor\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored.  \n",
    "텐서 속성은 모양, 데이터 유형 및 저장된 장치를 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Tensors 텐서에 대한 연산\n",
    "\n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing,\n",
    "indexing, slicing), sampling and more are\n",
    "comprehensively described [here](https://pytorch.org/docs/stable/torch.html)_.  \n",
    "산술, 선형 대수, 행렬 조작(트랜스포징, 인덱싱, 슬라이싱), 샘플링 등을 포함한 100개 이상의 텐서 연산이 링크에 포괄적으로 설명되어 있음\n",
    "\n",
    "Each of these operations can be run on the GPU (at typically higher speeds than on a CPU).  \n",
    "이러한 각 작업은 GPU(일반적으로 CPU보다 더 빠른 속도)에서 실행될 수 있음  \n",
    "\n",
    "If you’re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.  \n",
    "Colab을 사용하는 경우 Runtime > Change runtime type > GPU로 이동하여 GPU를 할당\n",
    "\n",
    "By default, tensors are created on the CPU.  \n",
    "기본적으로 CPU에 텐서가 생성됨  \n",
    "\n",
    "We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability).   \n",
    "우리는 명시적으로 텐서를 GPU로 이동시켜야 함 using .to method (GPU 가용성 확인 후)\n",
    "\n",
    "Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!  \n",
    "장치 전반에 걸쳐 대형 텐서를 복사하는 것은 시간과 메모리 측면에서 비용이 많이 들 수 있다는 것을 명심"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some of the operations작업 from the list.   \n",
    "\n",
    "If you’re familiar with the NumPy API, you’ll find the Tensor API a breeze to use.  \n",
    "NumPy API에 대해 잘 알고 있다면 Tensor API를 쉽게 사용할 수 있음  \n",
    "\n",
    "##### Standard numpy-like indexing and slicing:\n",
    "표준 Numpy-like 인덱싱 및 슬라이싱:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Joining tensors   \n",
    "\n",
    "You can use torch.cat to concatenate a sequence of tensors along a given dimension.  \n",
    "주어진 차원을 따라 일련의 텐서를 연결   \n",
    "\n",
    "See also ``torch.stack``, another tensor joining operator that is subtly different from `torch.cat`.  \n",
    "subtly 미묘한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4890, 0.4984, 0.3227, 0.7768, 0.4890, 0.4984, 0.3227, 0.7768, 0.4890,\n",
      "         0.4984, 0.3227, 0.7768],\n",
      "        [0.6828, 0.2250, 0.8209, 0.7472, 0.6828, 0.2250, 0.8209, 0.7472, 0.6828,\n",
      "         0.2250, 0.8209, 0.7472],\n",
      "        [0.9573, 0.8825, 0.5057, 0.1998, 0.9573, 0.8825, 0.5057, 0.1998, 0.9573,\n",
      "         0.8825, 0.5057, 0.1998]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4890, 0.4984, 0.3227, 0.7768],\n",
      "         [0.4890, 0.4984, 0.3227, 0.7768],\n",
      "         [0.4890, 0.4984, 0.3227, 0.7768]],\n",
      "\n",
      "        [[0.6828, 0.2250, 0.8209, 0.7472],\n",
      "         [0.6828, 0.2250, 0.8209, 0.7472],\n",
      "         [0.6828, 0.2250, 0.8209, 0.7472]],\n",
      "\n",
      "        [[0.9573, 0.8825, 0.5057, 0.1998],\n",
      "         [0.9573, 0.8825, 0.5057, 0.1998],\n",
      "         [0.9573, 0.8825, 0.5057, 0.1998]]])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.stack([tensor, tensor, tensor], dim=1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Arithmetic operations 산술연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single-element tensors \n",
    "\n",
    "If you have a one-element tensor, for example by aggregating all values of a tensor into one value,   \n",
    "예를 들어, 텐서의 모든 값을 하나의 값으로 집계하여 단일 요소 텐서를 갖는 경우,  \n",
    "\n",
    "you can convert it to a Python numerical value using item():  \n",
    "다음을 사용하여 그것을 파이썬 수치로 변환할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In-place operations \n",
    "\n",
    "Operations that store the result into the operand are called in-place.  \n",
    "결과를 피연산자에 저장하는 Operations를 in-place라고 함  \n",
    "\n",
    "They are denoted by a _ suffix.   \n",
    "For example: x.copy_(y), x.t_(), will change x.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE\n",
    "\n",
    "In-place operations save some memory,   \n",
    "In-place 연산은 일부 메모리를 절약하지만,  \n",
    "\n",
    "but can be problematic when computing derivatives because of an immediate loss of history.  \n",
    "즉각적인 이력 손실로 인해 도함수를 계산할 때 문제가 될 수 있음  \n",
    "\n",
    "Hence, their use is discouraged.\n",
    "따라서 사용이 권장되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge with NumPy\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.  \n",
    "CPU 및 NumPy 어레이의 텐서는 기본 메모리 위치를 공유할 수 있으며, 하나를 변경하면 다른 하나가 변경됨\n",
    "\n",
    "### Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A change in the tensor reflects in the NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in the NumPy array reflects in the tensor  \n",
    "넘파이 배열에서 변경하면 텐서에 반영됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

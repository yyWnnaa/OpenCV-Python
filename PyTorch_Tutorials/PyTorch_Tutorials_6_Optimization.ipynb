{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIMIZING MODEL PARAMETERS\n",
    "Now that we have a model and data it’s time to train,   \n",
    "validate and test our model by optimizing its parameters on our data.  \n",
    "데이터에 매개변수를 최적화하여 모델을 학습하고, 검증하고, 테스트  \n",
    "\n",
    "Training a model is an iterative process;   \n",
    "모델을 학습하는 과정은 반복적인 과정을 거침  \n",
    "\n",
    "in each iteration the model makes a guess about the output,   \n",
    "각 반복 단계에서 모델은 출력을 추측  \n",
    "\n",
    "calculates the error in its guess (loss),   \n",
    "추측과 정답 사이의 오류(손실(loss))를 계산  \n",
    "\n",
    "collects the derivatives(도함수) of the error with respect to its parameters   \n",
    "(as we saw in the previous section),   \n",
    "매개변수에 대한 오류의 도함수(derivative)를 수집  \n",
    "\n",
    "and optimizes these parameters using gradient descent.   \n",
    "경사하강법을 사용하여 이 파라미터들을 최적화  \n",
    "\n",
    "For a more detailed walkthrough of this process,   \n",
    "check out this video on backpropagation from 3Blue1Brown.  \n",
    "https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
    "\n",
    "### Prerequisite(기본) Code\n",
    "We load the code from the previous sections on Datasets & DataLoaders and Build Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Hyperparameters are adjustable parameters that let you control the model optimization process.  \n",
    "모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수  \n",
    "\n",
    "Different hyperparameter values can impact model training and convergence rates  \n",
    "서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율(convergence rate)에 영향을 미칠 수 있음  \n",
    "\n",
    "([read more](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)_ about hyperparameter tuning)  \n",
    "\n",
    "We define the following hyperparameters for training:\n",
    " - **Number of Epochs** - the number times to iterate over the dataset 데이터셋을 반복하는 횟수\n",
    " \n",
    " - **Batch Size** - the number of data samples propagated through the network before the parameters are updated   \n",
    " 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n",
    " \n",
    " - **Learning Rate** - how much to update models parameters at each batch/epoch.   \n",
    " Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.   \n",
    " 각 배치/에폭에서 모델의 매개변수를 조절하는 비율.   \n",
    " 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있음  \n",
    " 예측할 수 없는 동작 -> 표현이 애매하게 되었는데 / 진동 혹은 발산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "\n",
    "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop.   \n",
    "하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있음  \n",
    "\n",
    "Each iteration of the optimization loop is called an **epoch**.  \n",
    "최적화 단계의 각 반복(iteration)을 epoch라고 부름  \n",
    "\n",
    "Each epoch consists of two main parts:\n",
    " - **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.  \n",
    " 학습용 데이터셋을 반복(iterate)하고 최적의 매개변수로 수렴하기를 시도  \n",
    " \n",
    " - **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.  \n",
    " 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복(iterate)  \n",
    "\n",
    "Let's briefly familiarize ourselves with some of the concepts used in the training loop.   \n",
    "Jump ahead to see the `full-impl-label` of the optimization loop.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "When presented with some training data, our untrained network is likely not to give the correct answer.   \n",
    "학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높음  \n",
    "\n",
    "**Loss function** measures the degree of dissimilarity of obtained result to the target value,  \n",
    "obtain 얻다, 구하다  \n",
    "얻은 결과와 목표 값 사이의 불일치 정도를 측정  \n",
    "\n",
    "and it is the loss function that we want to minimize during training.  \n",
    "훈련 중에 최소화하려는 손실 함수임  \n",
    "\n",
    "To calculate the loss we make a prediction using the inputs of our given data sample   \n",
    "and compare it against the true data label value.  \n",
    "손실을 계산하려면 주어진 데이터 샘플의 입력을 사용하여 예측을 만들고 그것을 실제 데이터 레이블 값과 비교  \n",
    "\n",
    "Common loss functions include [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error) for regression tasks,   \n",
    "nn.MSELoss(평균 제곱 오차(MSE; Mean Square Error) : 회귀 문제(regression task)에 사용  \n",
    "\n",
    "and [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood) for classification.  \n",
    "n.NLLLoss (음의 로그 우도(Negative Log Likelihood) :  분류(classification)에 사용  \n",
    "확률적 예측 평가하고 손실을 최소화  \n",
    "\n",
    "[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) combines ``nn.LogSoftmax`` and ``nn.NLLLoss``.  \n",
    "nn.LogSoftmax와 nn.NLLLoss를 합침\n",
    "\n",
    "We pass our model's output logits to ``nn.CrossEntropyLoss``,   \n",
    "which will normalize the logits and compute the prediction error.  \n",
    "모델의 출력 logit을 nn.CrossEntropyLoss에 전달하여 logit을 정규화하고 예측 오류를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Optimization is the process of adjusting(조절) model parameters to reduce model error in each training step.   \n",
    "\n",
    "**Optimization algorithms** define how this process is performed (in this example we use Stochastic Gradient Descent).  \n",
    "\n",
    "All optimization logic is encapsulated in  the ``optimizer`` object.   \n",
    "캡슐화됨, :객체의 자료와 행위를 하나로 묶고, 실제 구현 내용을 외부에 감추는 것\n",
    "\n",
    "Here, we use the SGD optimizer;   \n",
    "additionally, there are many [different optimizers](https://pytorch.org/docs/stable/optim.html)  \n",
    "\n",
    "available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.  \n",
    "\n",
    "We initialize the optimizer / by registering the model's parameters /   \n",
    "that need to be trained, / and passing in the learning rate hyperparameter.   \n",
    "학습하려는 모델의 매개변수와 학습률(learning rate) 하이퍼파라미터를 등록하여 옵티마이저를 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    " * Call ``optimizer.zero_grad()`` to reset the gradients of model parameters.  \n",
    " 모델 매개변수의 변화도를 재설정  \n",
    "  Gradients by default add up; to prevent double-counting,   \n",
    "  we explicitly zero them at each iteration.  \n",
    "  기본적으로 변화도는 더해지기(add up) 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정  \n",
    "\n",
    " * Backpropagate the prediction loss with a call to ``loss.backward()``.   \n",
    " 예측 손실(prediction loss)을 역전파  \n",
    " PyTorch deposits the gradients of the loss w.r.t. each parameter.  \n",
    " PyTorch는 각 매개변수에 대한 손실의 변화도를 저장  \n",
    "\n",
    " * Once we have our gradients, we call ``optimizer.step()``   \n",
    " to adjust the parameters by the gradients collected in the backward pass.  \n",
    " 역전파 단계에서 수집된 변화도로 매개변수를 조정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Full Implementation\n",
    "We define ``train_loop`` that loops over our optimization code,   \n",
    "\n",
    "and ``test_loop`` that evaluates the model's performance against our test data.  \n",
    "\n",
    "``train_loop`` : 최적화 코드를 반복하여 수행  \n",
    "\n",
    "``test_loop`` : 테스트 데이터로 모델의 성능을 측정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad(): # 가중치 계산을 안 해도 됨\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the loss function and optimizer, and pass it to train_loop and test_loop.   \n",
    "손실 함수와 옵티마이저를 초기화하고 train_loop와 test_loop에 전달\n",
    "\n",
    "Feel free / to increase the number of epochs / to track the model’s improving performance.  \n",
    "모델의 성능 향상을 알아보기 위해 자유롭게 에폭(epoch) 수를 증가시켜 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.308593  [   64/60000]\n",
      "loss: 2.293018  [ 6464/60000]\n",
      "loss: 2.276059  [12864/60000]\n",
      "loss: 2.266814  [19264/60000]\n",
      "loss: 2.249116  [25664/60000]\n",
      "loss: 2.231523  [32064/60000]\n",
      "loss: 2.230085  [38464/60000]\n",
      "loss: 2.194862  [44864/60000]\n",
      "loss: 2.192758  [51264/60000]\n",
      "loss: 2.168314  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 2.154566 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.167217  [   64/60000]\n",
      "loss: 2.151591  [ 6464/60000]\n",
      "loss: 2.097949  [12864/60000]\n",
      "loss: 2.113068  [19264/60000]\n",
      "loss: 2.055542  [25664/60000]\n",
      "loss: 2.012326  [32064/60000]\n",
      "loss: 2.025604  [38464/60000]\n",
      "loss: 1.941904  [44864/60000]\n",
      "loss: 1.952305  [51264/60000]\n",
      "loss: 1.890079  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.877630 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.908119  [   64/60000]\n",
      "loss: 1.875005  [ 6464/60000]\n",
      "loss: 1.759864  [12864/60000]\n",
      "loss: 1.804685  [19264/60000]\n",
      "loss: 1.697069  [25664/60000]\n",
      "loss: 1.656414  [32064/60000]\n",
      "loss: 1.673217  [38464/60000]\n",
      "loss: 1.566543  [44864/60000]\n",
      "loss: 1.595147  [51264/60000]\n",
      "loss: 1.504720  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.514548 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.575490  [   64/60000]\n",
      "loss: 1.543826  [ 6464/60000]\n",
      "loss: 1.396998  [12864/60000]\n",
      "loss: 1.471151  [19264/60000]\n",
      "loss: 1.358403  [25664/60000]\n",
      "loss: 1.357294  [32064/60000]\n",
      "loss: 1.368933  [38464/60000]\n",
      "loss: 1.283491  [44864/60000]\n",
      "loss: 1.317881  [51264/60000]\n",
      "loss: 1.232847  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.253236 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.321998  [   64/60000]\n",
      "loss: 1.309206  [ 6464/60000]\n",
      "loss: 1.144938  [12864/60000]\n",
      "loss: 1.252137  [19264/60000]\n",
      "loss: 1.130663  [25664/60000]\n",
      "loss: 1.159661  [32064/60000]\n",
      "loss: 1.177809  [38464/60000]\n",
      "loss: 1.102372  [44864/60000]\n",
      "loss: 1.141280  [51264/60000]\n",
      "loss: 1.070004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.087367 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.150947  [   64/60000]\n",
      "loss: 1.158117  [ 6464/60000]\n",
      "loss: 0.974485  [12864/60000]\n",
      "loss: 1.111328  [19264/60000]\n",
      "loss: 0.989313  [25664/60000]\n",
      "loss: 1.025987  [32064/60000]\n",
      "loss: 1.058541  [38464/60000]\n",
      "loss: 0.984911  [44864/60000]\n",
      "loss: 1.025339  [51264/60000]\n",
      "loss: 0.966508  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.979198 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.031565  [   64/60000]\n",
      "loss: 1.059182  [ 6464/60000]\n",
      "loss: 0.856513  [12864/60000]\n",
      "loss: 1.016786  [19264/60000]\n",
      "loss: 0.901447  [25664/60000]\n",
      "loss: 0.932647  [32064/60000]\n",
      "loss: 0.981026  [38464/60000]\n",
      "loss: 0.907518  [44864/60000]\n",
      "loss: 0.944826  [51264/60000]\n",
      "loss: 0.897247  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.905577 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.943966  [   64/60000]\n",
      "loss: 0.990698  [ 6464/60000]\n",
      "loss: 0.772405  [12864/60000]\n",
      "loss: 0.950094  [19264/60000]\n",
      "loss: 0.843635  [25664/60000]\n",
      "loss: 0.864905  [32064/60000]\n",
      "loss: 0.927202  [38464/60000]\n",
      "loss: 0.854883  [44864/60000]\n",
      "loss: 0.886599  [51264/60000]\n",
      "loss: 0.847963  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.852830 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.876741  [   64/60000]\n",
      "loss: 0.939745  [ 6464/60000]\n",
      "loss: 0.709958  [12864/60000]\n",
      "loss: 0.900861  [19264/60000]\n",
      "loss: 0.802716  [25664/60000]\n",
      "loss: 0.814462  [32064/60000]\n",
      "loss: 0.886831  [38464/60000]\n",
      "loss: 0.817774  [44864/60000]\n",
      "loss: 0.842831  [51264/60000]\n",
      "loss: 0.810799  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.812948 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.822921  [   64/60000]\n",
      "loss: 0.899016  [ 6464/60000]\n",
      "loss: 0.661623  [12864/60000]\n",
      "loss: 0.862728  [19264/60000]\n",
      "loss: 0.771716  [25664/60000]\n",
      "loss: 0.775780  [32064/60000]\n",
      "loss: 0.854363  [38464/60000]\n",
      "loss: 0.790057  [44864/60000]\n",
      "loss: 0.808589  [51264/60000]\n",
      "loss: 0.781043  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.781237 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
